{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79cf6c46-6d83-40f6-aba9-51f3cfe37da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import re # For parsing filenames more robustly\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Add SuperGlue directory to Python path\n",
    "superglue_path = Path('SuperGluePretrainedNetwork')\n",
    "sys.path.append(str(superglue_path))\n",
    "\n",
    "# Import SuperGlue model components (adjust based on actual file structure if needed)\n",
    "from models.matching import Matching\n",
    "from models.utils import (frame2tensor, make_matching_plot_fast, AverageTimer, read_image)\n",
    "\n",
    "# Setup device (GPU if available, otherwise CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# SuperGlue configuration (using outdoor weights as they are generally robust)\n",
    "config = {\n",
    "    'superpoint': {\n",
    "        'nms_radius': 4,\n",
    "        'keypoint_threshold': 0.005,\n",
    "        'max_keypoints': -1 # Use -1 for no limit, or set a number like 1024 or 2048\n",
    "    },\n",
    "    'superglue': {\n",
    "        'weights': 'outdoor', # 'indoor' or 'outdoor'\n",
    "        'sinkhorn_iterations': 20,\n",
    "        'match_threshold': 0.2,\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Constants ---\n",
    "QUERY_DIR = Path('./extended_dataset/camera image')\n",
    "REF_DIR = Path('./extended_dataset/reference image')\n",
    "REFERENCE_IMAGE_SIZE = (1000, 1000) # Assuming all reference images are 1000x1000\n",
    "VIZ_COUNT = 5 # Number of results to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161eb941-50b4-4cac-a787-489dbc4626a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Function to Parse Filename\n",
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Parses the filename to extract ground truth coordinates and angle.\n",
    "\n",
    "    Filename format: (ignore)_(x,y)_angle_(ignore).png\n",
    "    Coordinates: (x, y) relative to reference center. +x down, +y right.\n",
    "    Angle: Anti-clockwise rotation degrees (0-360).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = filename.stem.split('_') # Use stem to remove .png\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Warning: Skipping poorly formatted file: {filename.name}\")\n",
    "            return None\n",
    "\n",
    "        # Robustly find the coordinate tuple part like '(float,float)'\n",
    "        coord_match = re.search(r'\\((-?\\d+\\.?\\d*),(-?\\d+\\.?\\d*)\\)', parts[1])\n",
    "        if not coord_match:\n",
    "            # Try finding it in the second part if the first was just coords like '507,4801'\n",
    "             coord_match = re.search(r'\\((-?\\d+\\.?\\d*),(-?\\d+\\.?\\d*)\\)', parts[1] if len(parts) > 1 else '')\n",
    "             if not coord_match and len(parts) > 2:\n",
    "                 coord_match = re.search(r'\\((-?\\d+\\.?\\d*),(-?\\d+\\.?\\d*)\\)', parts[2])\n",
    "\n",
    "        if not coord_match:\n",
    "             # Handle case where coords might not have parentheses like '97.86,-113.64'\n",
    "             coord_parts = parts[1].split(',')\n",
    "             if len(coord_parts) == 2:\n",
    "                 try:\n",
    "                     gt_x = float(coord_parts[0])\n",
    "                     gt_y = float(coord_parts[1])\n",
    "                 except ValueError:\n",
    "                     print(f\"Warning: Could not parse coordinates from {parts[1]} in file: {filename.name}\")\n",
    "                     return None\n",
    "             else: # Try the next part\n",
    "                 if len(parts) > 2:\n",
    "                     coord_parts = parts[2].split(',')\n",
    "                     if len(coord_parts) == 2:\n",
    "                         try:\n",
    "                             gt_x = float(coord_parts[0])\n",
    "                             gt_y = float(coord_parts[1])\n",
    "                         except ValueError:\n",
    "                             print(f\"Warning: Could not parse coordinates from {parts[2]} in file: {filename.name}\")\n",
    "                             return None\n",
    "                     else:\n",
    "                          print(f\"Warning: Could not find coordinate tuple in file: {filename.name}\")\n",
    "                          return None\n",
    "                 else:\n",
    "                      print(f\"Warning: Could not find coordinate tuple in file: {filename.name}\")\n",
    "                      return None\n",
    "\n",
    "        else:\n",
    "            gt_x = float(coord_match.group(1))\n",
    "            gt_y = float(coord_match.group(2))\n",
    "\n",
    "\n",
    "        # Find the angle part (float) - usually the part after coordinates\n",
    "        angle_part_index = -1\n",
    "        for i in range(1, len(parts) -1): # Check parts between ignore and location\n",
    "             # Check if it looks like the coordinate tuple we just found\n",
    "            is_coord_part = False\n",
    "            if coord_match:\n",
    "                 if parts[i] == f\"({coord_match.group(1)},{coord_match.group(2)})\":\n",
    "                     is_coord_part = True\n",
    "            elif len(parts[i].split(',')) == 2: # Check if it matches the non-parenthesis coords\n",
    "                 try:\n",
    "                     float(parts[i].split(',')[0])\n",
    "                     float(parts[i].split(',')[1])\n",
    "                     is_coord_part = True\n",
    "                 except ValueError:\n",
    "                     pass\n",
    "\n",
    "            if is_coord_part and i + 1 < len(parts) -1 : # If this was the coord part, angle should be next\n",
    "                 try:\n",
    "                     gt_angle = float(parts[i+1])\n",
    "                     angle_part_index = i+1\n",
    "                     break\n",
    "                 except ValueError:\n",
    "                     pass # Continue searching\n",
    "\n",
    "        # If not found after coordinates, try finding the first float number after the first underscore\n",
    "        if angle_part_index == -1:\n",
    "             for i in range(1, len(parts) - 1):\n",
    "                  try:\n",
    "                      gt_angle = float(parts[i])\n",
    "                      # Avoid accidentally parsing the first coordinate if it's simple like '(500)'\n",
    "                      if not parts[i].startswith('(') or not parts[i].endswith(')'):\n",
    "                          angle_part_index = i\n",
    "                          break\n",
    "                  except ValueError:\n",
    "                     continue # Not a float, try next part\n",
    "\n",
    "        if angle_part_index == -1:\n",
    "             print(f\"Warning: Could not find angle value in file: {filename.name}\")\n",
    "             return None\n",
    "\n",
    "\n",
    "        # Validate coordinate ranges (optional but good practice)\n",
    "        # if not (-500 <= gt_x <= 500 and -500 <= gt_y <= 500):\n",
    "        #     print(f\"Warning: Coordinates out of range [-500, 500] in file: {filename.name}\")\n",
    "            # return None # Or just continue if you want to process them anyway\n",
    "\n",
    "        # Validate angle range\n",
    "        if not (0 <= gt_angle <= 360):\n",
    "             # Allow for slight floating point inaccuracies near 0/360\n",
    "             if not (abs(gt_angle) < 1e-9 or abs(gt_angle - 360) < 1e-9):\n",
    "                 print(f\"Warning: Angle out of range [0, 360] in file: {filename.name}\")\n",
    "                 # Handle wrap around? e.g., 360.1 -> 0.1? or -0.1 -> 359.9?\n",
    "                 gt_angle = gt_angle % 360\n",
    "                 print(f\"         Adjusted angle to {gt_angle:.2f}\")\n",
    "                 #return None # Or adjust angle: gt_angle = gt_angle % 360\n",
    "\n",
    "        return {'x': gt_x, 'y': gt_y, 'angle': gt_angle}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing filename {filename.name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cd7f3f-0ee9-48ff-b283-043e2805c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Function to Estimate Transformation\n",
    "def estimate_transform(kp0, kp1, matches, confidence):\n",
    "    \"\"\"\n",
    "    Estimates rotation and translation from keypoint matches using RANSAC.\n",
    "    Returns:\n",
    "        - M: 2x3 Affine transformation matrix (or None if failed)\n",
    "        - angle_deg: Estimated rotation in degrees (anti-clockwise, 0-360)\n",
    "        - trans_px: Estimated translation (tx, ty) in pixels (relative to top-left)\n",
    "        - mask: RANSAC inlier mask\n",
    "    \"\"\"\n",
    "    mkpts0 = kp0[matches[:, 0]]\n",
    "    mkpts1 = kp1[matches[:, 1]]\n",
    "\n",
    "    # Filter based on confidence\n",
    "    min_req_matches = 4 # Minimum matches for estimateAffine2D\n",
    "    indices = np.where(confidence > config['superglue']['match_threshold'])[0]\n",
    "    mkpts0 = mkpts0[indices]\n",
    "    mkpts1 = mkpts1[indices]\n",
    "\n",
    "    if mkpts0.shape[0] < min_req_matches:\n",
    "        # print(f\"Not enough confident matches ({mkpts0.shape[0]})\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Use cv2.estimateAffine2D for rotation, translation, and scale\n",
    "    # It requires at least 3 points, RANSAC helps with outliers.\n",
    "    M, mask = cv2.estimateAffine2D(mkpts0, mkpts1, method=cv2.RANSAC,\n",
    "                                     ransacReprojThreshold=5.0) # Adjust threshold as needed\n",
    "\n",
    "    if M is None or mask is None:\n",
    "        # print(\"RANSAC failed to find a transformation.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Check number of inliers\n",
    "    num_inliers = np.sum(mask)\n",
    "    if num_inliers < min_req_matches:\n",
    "        # print(f\"Not enough RANSAC inliers ({num_inliers})\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "    # Decompose the Affine matrix M = [[a, b, tx], [c, d, ty]]\n",
    "    # Assuming M = [[s*cos(theta), -s*sin(theta), tx], [s*sin(theta), s*cos(theta), ty]]\n",
    "    # where s is scale and theta is the rotation angle\n",
    "\n",
    "    # Extract translation\n",
    "    tx = M[0, 2]\n",
    "    ty = M[1, 2]\n",
    "    trans_px = (tx, ty)\n",
    "\n",
    "    # Extract rotation angle\n",
    "    # angle_rad = atan2(c, a) or atan2(-b, d) - use atan2(M[1,0], M[0,0])\n",
    "    angle_rad = math.atan2(M[1, 0], M[0, 0])\n",
    "    angle_deg_raw = np.degrees(angle_rad)\n",
    "\n",
    "    # Convert OpenCV angle (rotation of coordinate system) to anti-clockwise image rotation (0-360)\n",
    "    # A positive angle_rad from atan2(M[1,0], M[0,0]) corresponds to an anti-clockwise rotation\n",
    "    # of the coordinate axes. To get the image rotation (how much the object turned anti-clockwise),\n",
    "    # we usually take the negative, but let's verify convention.\n",
    "    # If query is rotated A degrees anti-clockwise relative to ref:\n",
    "    # Point p_q in query becomes p_r in reference.\n",
    "    # p_r = RotationMatrix(A) * p_q + Translation\n",
    "    # OpenCV's M transforms query points *to* reference points.\n",
    "    # So M[0,0] = s*cos(A), M[1,0] = s*sin(A).\n",
    "    # angle_rad = atan2(M[1,0], M[0,0]) = atan2(s*sin(A), s*cos(A)) = A\n",
    "    # So, the angle A is directly angle_rad. We need it in 0-360 range.\n",
    "    angle_deg = angle_deg_raw % 360\n",
    "    if angle_deg < 0:\n",
    "         angle_deg += 360 # Ensure positive range [0, 360)\n",
    "\n",
    "    # Optional: Extract scale (should be close to 1 if images are same scale)\n",
    "    # scale_x = np.sqrt(M[0, 0]**2 + M[1, 0]**2)\n",
    "    # scale_y = np.sqrt(M[0, 1]**2 + M[1, 1]**2)\n",
    "    # print(f\"Estimated scale: sx={scale_x:.2f}, sy={scale_y:.2f}\")\n",
    "\n",
    "    return M, angle_deg, trans_px, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1565cd-5753-46f1-b597-92e254f17d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Function for Coordinate Conversion\n",
    "def convert_to_relative_center_coords(trans_px_tl, angle_deg, query_shape, ref_shape):\n",
    "    \"\"\"\n",
    "    Converts the translation relative to top-left corners and rotation\n",
    "    to the translation relative to image centers, using the user's convention.\n",
    "\n",
    "    Args:\n",
    "        trans_px_tl (tuple): (tx, ty) translation from query top-left to ref top-left (OpenCV standard).\n",
    "        angle_deg (float): Anti-clockwise rotation angle in degrees.\n",
    "        query_shape (tuple): (height, width) of the query image.\n",
    "        ref_shape (tuple): (height, width) of the reference image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rel_x, rel_y) relative center coordinates (+x down, +y right).\n",
    "               Returns (None, None) if input is invalid.\n",
    "    \"\"\"\n",
    "    if trans_px_tl is None or angle_deg is None:\n",
    "        return None, None\n",
    "\n",
    "    q_h, q_w = query_shape[:2]\n",
    "    r_h, r_w = ref_shape[:2]\n",
    "\n",
    "    # Reference image center (in OpenCV coords: origin top-left, +x right, +y down)\n",
    "    ref_center_cv_x = r_w / 2.0\n",
    "    ref_center_cv_y = r_h / 2.0\n",
    "\n",
    "    # Query image center (in its own local coords: origin top-left)\n",
    "    query_center_local_x = q_w / 2.0\n",
    "    query_center_local_y = q_h / 2.0\n",
    "\n",
    "    # Calculate where the query center ends up in the reference image coordinate system (origin top-left)\n",
    "    # We need the full affine matrix M for this, as simple translation isn't enough due to rotation.\n",
    "    # Let's re-calculate M using the estimated components (assuming scale=1)\n",
    "    angle_rad = np.radians(angle_deg)\n",
    "    cos_a = np.cos(angle_rad)\n",
    "    sin_a = np.sin(angle_rad)\n",
    "    tx, ty = trans_px_tl\n",
    "\n",
    "    M = np.array([\n",
    "        [cos_a, -sin_a, tx],\n",
    "        [sin_a,  cos_a, ty]\n",
    "    ])\n",
    "\n",
    "    # Transform the query center using the affine matrix M\n",
    "    # ref_coords = M @ [query_local_x, query_local_y, 1]\n",
    "    query_center_in_ref_x = M[0, 0] * query_center_local_x + M[0, 1] * query_center_local_y + M[0, 2]\n",
    "    query_center_in_ref_y = M[1, 0] * query_center_local_x + M[1, 1] * query_center_local_y + M[1, 2]\n",
    "\n",
    "    # Now, find the vector from the reference center to the query center (in OpenCV coords)\n",
    "    vec_cv_x = query_center_in_ref_x - ref_center_cv_x\n",
    "    vec_cv_y = query_center_in_ref_y - ref_center_cv_y\n",
    "\n",
    "    # Convert this vector to the user's coordinate system (+x down, +y right)\n",
    "    rel_x = vec_cv_y # User's x is OpenCV's y\n",
    "    rel_y = vec_cv_x # User's y is OpenCV's x\n",
    "\n",
    "    return rel_x, rel_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada63a64-1a48-4184-be05-0f136b078661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SuperGlue model...\n",
      "Loaded SuperPoint model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jain/Desktop/IITK/Project_EE798T/SuperGluePretrainedNetwork/models/superpoint.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n",
      "/home/jain/Desktop/IITK/Project_EE798T/SuperGluePretrainedNetwork/models/superglue.py:226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(str(path)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Model\n",
    "print(\"Loading SuperGlue model...\")\n",
    "matching = Matching(config).eval().to(device)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0966fd7c-ac09-4208-af04-99bb36620bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2040 query images. Processing...\n",
      "\n",
      "Processed 2040 out of 2040 files.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main Processing Loop\n",
    "results = []\n",
    "all_filenames = []\n",
    "\n",
    "# Ensure directories exist\n",
    "if not QUERY_DIR.is_dir() or not REF_DIR.is_dir():\n",
    "    raise FileNotFoundError(\"Query or Reference image directory not found.\")\n",
    "\n",
    "# Get list of query images, assuming reference images have the same names\n",
    "# Ignore hidden files like .DS_Store\n",
    "query_files = sorted([f for f in QUERY_DIR.iterdir() if f.is_file() and not f.name.startswith('.') and f.suffix.lower() == '.png'])\n",
    "\n",
    "if not query_files:\n",
    "    print(f\"No PNG files found in {QUERY_DIR}\")\n",
    "else:\n",
    "    print(f\"Found {len(query_files)} query images. Processing...\")\n",
    "\n",
    "for query_path in query_files:\n",
    "    filename = query_path.name\n",
    "    ref_path = REF_DIR / filename\n",
    "\n",
    "    if not ref_path.exists():\n",
    "        print(f\"Warning: Reference image not found for {filename}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 1. Parse Ground Truth\n",
    "    ground_truth = parse_filename(query_path)\n",
    "    if ground_truth is None:\n",
    "        continue # Skip if parsing failed\n",
    "\n",
    "    try:\n",
    "        # 2. Load Images (using read_image from SuperGlue utils for consistency)\n",
    "        # read_image loads in grayscale and converts to float tensor [0,1]\n",
    "        image0, inp0, scales0 = read_image(str(query_path), device, [-1], 0, False) # Query\n",
    "        image1, inp1, scales1 = read_image(str(ref_path), device, [-1], 0, False)   # Reference\n",
    "\n",
    "        if image0 is None or image1 is None:\n",
    "             print(f\"Warning: Could not load images for {filename}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        query_h, query_w = image0.shape[:2]\n",
    "        ref_h, ref_w = image1.shape[:2]\n",
    "\n",
    "        # Check if reference image size is as expected\n",
    "        if ref_h != REFERENCE_IMAGE_SIZE[0] or ref_w != REFERENCE_IMAGE_SIZE[1]:\n",
    "            print(f\"Warning: Reference image {filename} has unexpected size {image1.shape[:2]}. Expected {REFERENCE_IMAGE_SIZE}. Adjusting calculations.\")\n",
    "            # You might need to resize or handle this differently if sizes vary significantly\n",
    "\n",
    "        # 3. Perform Matching\n",
    "        with torch.no_grad():\n",
    "            pred = matching({'image0': inp0, 'image1': inp1})\n",
    "\n",
    "        # Detach tensors and move to CPU for NumPy/OpenCV processing\n",
    "        pred = {k: v[0].cpu().numpy() for k, v in pred.items()}\n",
    "        kpts0, kpts1 = pred['keypoints0'], pred['keypoints1']\n",
    "        matches, conf = pred['matches0'], pred['matching_scores0']\n",
    "\n",
    "        # Keep only valid matches (index != -1)\n",
    "        valid = matches > -1\n",
    "        mkpts0_idx = np.where(valid)[0]\n",
    "        mkpts1_idx = matches[valid]\n",
    "        conf = conf[valid]\n",
    "        # Create the match array format expected by estimate_transform: Nx2 array of indices\n",
    "        match_indices = np.stack([mkpts0_idx, mkpts1_idx], axis=1)\n",
    "\n",
    "        # 4. Estimate Transformation\n",
    "        M, pred_angle, trans_px_tl, _ = estimate_transform(kpts0, kpts1, match_indices, conf)\n",
    "\n",
    "        # 5. Convert Coordinates\n",
    "        if M is not None:\n",
    "            pred_x, pred_y = convert_to_relative_center_coords(trans_px_tl, pred_angle, (query_h, query_w), (ref_h, ref_w))\n",
    "        else:\n",
    "            pred_x, pred_y, pred_angle = None, None, None # Indicate failure\n",
    "\n",
    "        # 6. Store Results\n",
    "        results.append({\n",
    "            'filename': filename,\n",
    "            'query_path': str(query_path),\n",
    "            'ref_path': str(ref_path),\n",
    "            'gt_x': ground_truth['x'],\n",
    "            'gt_y': ground_truth['y'],\n",
    "            'gt_angle': ground_truth['angle'],\n",
    "            'pred_x': pred_x,\n",
    "            'pred_y': pred_y,\n",
    "            'pred_angle': pred_angle,\n",
    "            'query_shape': (query_h, query_w),\n",
    "            'ref_shape': (ref_h, ref_w),\n",
    "            # Store intermediate results for visualization if needed\n",
    "            'kpts0': kpts0,\n",
    "            'kpts1': kpts1,\n",
    "            'matches': match_indices,\n",
    "            'confidence': conf\n",
    "        })\n",
    "        all_filenames.append(filename) # Keep track even if prediction failed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        # Optionally store failure information\n",
    "        results.append({\n",
    "            'filename': filename,\n",
    "            'query_path': str(query_path),\n",
    "            'ref_path': str(ref_path),\n",
    "            'gt_x': ground_truth.get('x'), # Use .get in case parsing failed earlier\n",
    "            'gt_y': ground_truth.get('y'),\n",
    "            'gt_angle': ground_truth.get('angle'),\n",
    "            'pred_x': None,\n",
    "            'pred_y': None,\n",
    "            'pred_angle': None,\n",
    "            'error': str(e)\n",
    "        })\n",
    "        all_filenames.append(filename)\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} out of {len(query_files)} files.\")\n",
    "# If you wanted train/test split, you could use sklearn here on 'all_filenames'\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_files, test_files = train_test_split(all_filenames, test_size=0.2, random_state=42)\n",
    "# Then filter the 'results' list based on these filenames.\n",
    "# For now, we evaluate on all processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7be3ad-859f-4ea0-abb0-5474e4ca16d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Accuracy Metrics (867 successful predictions) ---\n",
      "Coordinate Error (pixels):\n",
      "  Mean:   33.953\n",
      "  Median: 0.806\n",
      "  StdDev: 226.254\n",
      "\n",
      "Angle Error (degrees):\n",
      "  Mean:   8.282\n",
      "  Median: 0.034\n",
      "  StdDev: 33.833\n",
      "Total files processed: 2040\n",
      "Prediction failure rate: 57.50%\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Calculate Accuracy Metrics\n",
    "coord_errors = []\n",
    "angle_errors = []\n",
    "successful_predictions = 0\n",
    "\n",
    "for res in results:\n",
    "    if res['pred_x'] is not None and res['pred_y'] is not None and res['pred_angle'] is not None:\n",
    "        successful_predictions += 1\n",
    "\n",
    "        # Coordinate Error (Euclidean Distance)\n",
    "        dx = res['pred_x'] - res['gt_x']\n",
    "        dy = res['pred_y'] - res['gt_y']\n",
    "        coord_err = np.sqrt(dx**2 + dy**2)\n",
    "        coord_errors.append(coord_err)\n",
    "\n",
    "        # Angle Error (handle wrap-around)\n",
    "        angle_diff = abs(res['pred_angle'] - res['gt_angle'])\n",
    "        angle_err = min(angle_diff, 360.0 - angle_diff)\n",
    "        angle_errors.append(angle_err)\n",
    "\n",
    "print(f\"\\n--- Accuracy Metrics ({successful_predictions} successful predictions) ---\")\n",
    "if successful_predictions > 0:\n",
    "    mean_coord_error = np.mean(coord_errors)\n",
    "    median_coord_error = np.median(coord_errors)\n",
    "    std_coord_error = np.std(coord_errors)\n",
    "\n",
    "    mean_angle_error = np.mean(angle_errors)\n",
    "    median_angle_error = np.median(angle_errors)\n",
    "    std_angle_error = np.std(angle_errors)\n",
    "\n",
    "    print(f\"Coordinate Error (pixels):\")\n",
    "    print(f\"  Mean:   {mean_coord_error:.3f}\")\n",
    "    print(f\"  Median: {median_coord_error:.3f}\")\n",
    "    print(f\"  StdDev: {std_coord_error:.3f}\")\n",
    "\n",
    "    print(f\"\\nAngle Error (degrees):\")\n",
    "    print(f\"  Mean:   {mean_angle_error:.3f}\")\n",
    "    print(f\"  Median: {median_angle_error:.3f}\")\n",
    "    print(f\"  StdDev: {std_angle_error:.3f}\")\n",
    "\n",
    "    # Optional: Print quantiles or histogram for better error distribution understanding\n",
    "    # print(f\"\\nCoord Error Quantiles (px): {np.quantile(coord_errors, [0.25, 0.5, 0.75, 0.9, 0.95])}\")\n",
    "    # print(f\"Angle Error Quantiles (deg): {np.quantile(angle_errors, [0.25, 0.5, 0.75, 0.9, 0.95])}\")\n",
    "\n",
    "    # plt.figure(figsize=(12, 5))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.hist(coord_errors, bins=50)\n",
    "    # plt.title('Coordinate Error Distribution')\n",
    "    # plt.xlabel('Error (pixels)')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.hist(angle_errors, bins=50)\n",
    "    # plt.title('Angle Error Distribution')\n",
    "    # plt.xlabel('Error (degrees)')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No successful predictions were made.\")\n",
    "\n",
    "print(f\"Total files processed: {len(results)}\")\n",
    "print(f\"Prediction failure rate: {(len(results) - successful_predictions) / len(results) * 100 if len(results) > 0 else 0:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbc7650-1ffc-4250-8d7a-13e068ad5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization Function\n",
    "def get_rotated_rect_corners(center_x, center_y, w, h, angle_deg, ref_shape):\n",
    "    \"\"\"\n",
    "    Calculates the four corner points of a rectangle rotated around its center.\n",
    "    Center coordinates are relative to the reference image center (+x down, +y right).\n",
    "    Angle is anti-clockwise degrees.\n",
    "    Returns corners in OpenCV format (integer tuples, origin top-left).\n",
    "    \"\"\"\n",
    "    r_h, r_w = ref_shape[:2]\n",
    "    ref_center_cv_x = r_w / 2.0\n",
    "    ref_center_cv_y = r_h / 2.0\n",
    "\n",
    "    # Convert user center coords back to OpenCV top-left origin coords\n",
    "    center_cv_x = center_y + ref_center_cv_x # User y is CV x\n",
    "    center_cv_y = center_x + ref_center_cv_y # User x is CV y\n",
    "\n",
    "    # Angle for rotation matrix (negative for rotating points)\n",
    "    angle_rad = np.radians(-angle_deg) # Need to rotate points clockwise if angle is anti-clockwise image rotation\n",
    "    cos_a = np.cos(angle_rad)\n",
    "    sin_a = np.sin(angle_rad)\n",
    "\n",
    "    # Rectangle corners relative to its center (0,0)\n",
    "    hw = w / 2.0\n",
    "    hh = h / 2.0\n",
    "    corners_local = np.array([\n",
    "        [-hw, -hh], # Top-left\n",
    "        [ hw, -hh], # Top-right\n",
    "        [ hw,  hh], # Bottom-right\n",
    "        [-hw,  hh]  # Bottom-left\n",
    "    ])\n",
    "\n",
    "    # Rotate and translate corners\n",
    "    corners_rotated = np.zeros_like(corners_local)\n",
    "    for i, (x, y) in enumerate(corners_local):\n",
    "        x_rot = x * cos_a - y * sin_a\n",
    "        y_rot = x * sin_a + y * cos_a\n",
    "        corners_rotated[i, 0] = x_rot + center_cv_x\n",
    "        corners_rotated[i, 1] = y_rot + center_cv_y\n",
    "\n",
    "    # Return as integer tuples\n",
    "    return corners_rotated.astype(int)\n",
    "\n",
    "\n",
    "def visualize_result(result_data, border_thickness=3):\n",
    "    \"\"\"\n",
    "    Visualizes the query, reference, and localization boxes (predicted vs ground truth).\n",
    "    \"\"\"\n",
    "    query_img_bgr = cv2.imread(result_data['query_path'])\n",
    "    ref_img_bgr = cv2.imread(result_data['ref_path'])\n",
    "\n",
    "    if query_img_bgr is None or ref_img_bgr is None:\n",
    "        print(f\"Error loading images for visualization: {result_data['filename']}\")\n",
    "        return\n",
    "\n",
    "    q_h, q_w = result_data['query_shape'][:2]\n",
    "    ref_shape = result_data['ref_shape']\n",
    "\n",
    "    # Draw Ground Truth Box (Green)\n",
    "    gt_corners = get_rotated_rect_corners(\n",
    "        result_data['gt_x'], result_data['gt_y'],\n",
    "        q_w, q_h, result_data['gt_angle'], ref_shape\n",
    "    )\n",
    "    cv2.polylines(ref_img_bgr, [gt_corners], isClosed=True, color=(0, 255, 0), thickness=border_thickness) # Green\n",
    "\n",
    "    # Draw Predicted Box (Red) - only if prediction was successful\n",
    "    if result_data['pred_x'] is not None:\n",
    "        pred_corners = get_rotated_rect_corners(\n",
    "            result_data['pred_x'], result_data['pred_y'],\n",
    "            q_w, q_h, result_data['pred_angle'], ref_shape\n",
    "        )\n",
    "        cv2.polylines(ref_img_bgr, [pred_corners], isClosed=True, color=(0, 0, 255), thickness=border_thickness) # Red\n",
    "        pred_status = \"Successful\"\n",
    "        coord_err = np.sqrt((result_data['pred_x'] - result_data['gt_x'])**2 + (result_data['pred_y'] - result_data['gt_y'])**2)\n",
    "        angle_diff = abs(result_data['pred_angle'] - result_data['gt_angle'])\n",
    "        angle_err = min(angle_diff, 360.0 - angle_diff)\n",
    "        error_text = f\"Coord Err: {coord_err:.2f}px, Angle Err: {angle_err:.2f}deg\"\n",
    "    else:\n",
    "        pred_status = \"Failed\"\n",
    "        error_text = \"Prediction Failed\"\n",
    "\n",
    "    # --- Display using Matplotlib ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Display Query Image\n",
    "    axes[0].imshow(cv2.cvtColor(query_img_bgr, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(f\"Query: {result_data['filename']}\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Display Reference Image with Boxes\n",
    "    axes[1].imshow(cv2.cvtColor(ref_img_bgr, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f\"Reference (GT=Green, Pred=Red) - {pred_status}\")\n",
    "    axes[1].text(5, 25, error_text, color='yellow', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Ground Truth: ({result_data['gt_x']:.2f}, {result_data['gt_y']:.2f}), {result_data['gt_angle']:.2f}°\\n\"\n",
    "                 f\"Prediction: ({result_data['pred_x']:.2f}, {result_data['pred_y']:.2f}), {result_data['pred_angle']:.2f}°\" if pred_status==\"Successful\" else \\\n",
    "                 f\"Ground Truth: ({result_data['gt_x']:.2f}, {result_data['gt_y']:.2f}), {result_data['gt_angle']:.2f}°\\nPrediction: Failed\", y=0.95)\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.9]) # Adjust layout to prevent title overlap\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc719627-6894-4178-b42a-2ff0c5e2ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separation and Error Calculation complete:\n",
      "  Found 867 successful predictions.\n",
      "  Found 1173 failed predictions.\n",
      "  Identified 54 successful predictions with high error.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8.5: Separate Results and Calculate Errors\n",
    "\n",
    "successful_results = []\n",
    "failed_results = []\n",
    "high_error_results = [] # New list\n",
    "\n",
    "# Define error thresholds\n",
    "COORD_ERROR_THRESHOLD = 50.0 # pixels\n",
    "ANGLE_ERROR_THRESHOLD = 20.0 # degrees\n",
    "\n",
    "if 'results' in locals() and isinstance(results, list):\n",
    "    for res in results:\n",
    "        is_successful = (res.get('pred_x') is not None and\n",
    "                         res.get('pred_y') is not None and\n",
    "                         res.get('pred_angle') is not None)\n",
    "\n",
    "        if is_successful:\n",
    "            # Calculate errors for successful predictions\n",
    "            dx = res['pred_x'] - res['gt_x']\n",
    "            dy = res['pred_y'] - res['gt_y']\n",
    "            coord_err = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "            angle_diff = abs(res['pred_angle'] - res['gt_angle'])\n",
    "            angle_err = min(angle_diff, 360.0 - angle_diff)\n",
    "\n",
    "            # Store errors back into the dictionary\n",
    "            res['coord_error'] = coord_err\n",
    "            res['angle_error'] = angle_err\n",
    "\n",
    "            successful_results.append(res) # Add to successful list\n",
    "\n",
    "            # Check if it qualifies for high error list\n",
    "            if coord_err > COORD_ERROR_THRESHOLD or angle_err > ANGLE_ERROR_THRESHOLD:\n",
    "                high_error_results.append(res)\n",
    "\n",
    "        else:\n",
    "            # Add placeholder errors for failed results if needed later, otherwise just add to list\n",
    "            res['coord_error'] = None\n",
    "            res['angle_error'] = None\n",
    "            failed_results.append(res)\n",
    "\n",
    "    print(f\"Separation and Error Calculation complete:\")\n",
    "    print(f\"  Found {len(successful_results)} successful predictions.\")\n",
    "    print(f\"  Found {len(failed_results)} failed predictions.\")\n",
    "    print(f\"  Identified {len(high_error_results)} successful predictions with high error.\")\n",
    "    # Optional: Sort high_error_results if desired (e.g., by coordinate error descending)\n",
    "    # high_error_results.sort(key=lambda x: x.get('coord_error', 0), reverse=True)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'results' variable not found or is not a list.\")\n",
    "    # Define empty lists to prevent errors later\n",
    "    successful_results = []\n",
    "    failed_results = []\n",
    "    high_error_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c10bf3fa-c971-47af-896e-6067edd5ccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to switch backend to inline.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8.7: Attempt to force inline backend before visualization\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Attempted to switch backend to inline.\")\n",
    "# Optional: re-import display function from IPython if needed, though usually not required for inline\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e6e636-8ae1-4083-8abd-20457444b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Visualizing up to 5 FAILED Prediction Examples ====================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'count_failed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNo failed prediction results found to visualize.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# ... (visualization loop for failed_results) ...\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinished displaying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_failed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed examples.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Make sure count_failed is defined in the loop\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# --- Visualize HIGH ERROR Successful Predictions ---\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Visualizing up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_VIZ_EACH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m HIGH ERROR Successful Prediction Examples \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_failed' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 9: Perform Visualization for Failed, High-Error Successful, and Generally Successful Examples\n",
    "\n",
    "MAX_VIZ_EACH = 5 # Number of examples to show for each category\n",
    "\n",
    "# --- Visualize FAILED Predictions ---\n",
    "print(f\"\\n{'='*20} Visualizing up to {MAX_VIZ_EACH} FAILED Prediction Examples {'='*20}\")\n",
    "# (Keep the existing code for visualizing failed_results here...)\n",
    "if 'failed_results' not in locals() or not isinstance(failed_results, list):\n",
    "     print(\"ERROR: 'failed_results' list not found. Did you run the cell above (Cell 8.5)?\")\n",
    "elif not failed_results:\n",
    "    print(\"\\nNo failed prediction results found to visualize.\")\n",
    "else:\n",
    "    # ... (visualization loop for failed_results) ...\n",
    "    print(f\"\\nFinished displaying {count_failed} failed examples.\") # Make sure count_failed is defined in the loop\n",
    "\n",
    "# --- Visualize HIGH ERROR Successful Predictions ---\n",
    "print(f\"\\n{'='*20} Visualizing up to {MAX_VIZ_EACH} HIGH ERROR Successful Prediction Examples {'='*20}\")\n",
    "print(f\"(Thresholds: Coord > {COORD_ERROR_THRESHOLD}px OR Angle > {ANGLE_ERROR_THRESHOLD}deg)\")\n",
    "\n",
    "if 'high_error_results' not in locals() or not isinstance(high_error_results, list):\n",
    "     print(\"ERROR: 'high_error_results' list not found. Did you run the cell above (Cell 8.5)?\")\n",
    "elif not high_error_results:\n",
    "    print(\"\\nNo successful predictions with high error found to visualize.\")\n",
    "else:\n",
    "    print(f\"\\nDisplaying the first {min(len(high_error_results), MAX_VIZ_EACH)} high-error successful examples...\")\n",
    "    count_hierr = 0\n",
    "    for res in high_error_results:\n",
    "        if count_hierr < MAX_VIZ_EACH:\n",
    "            # Display calculated errors clearly\n",
    "            coord_err_str = f\"{res.get('coord_error', 'N/A'):.2f}\" if res.get('coord_error') is not None else \"N/A\"\n",
    "            angle_err_str = f\"{res.get('angle_error', 'N/A'):.2f}\" if res.get('angle_error') is not None else \"N/A\"\n",
    "            print(f\"\\n[{count_hierr+1}/{min(len(high_error_results), MAX_VIZ_EACH)}] Visualizing HIGH ERROR result for: {res.get('filename', 'N/A')}\")\n",
    "            print(f\"  Errors - Coord: {coord_err_str}px, Angle: {angle_err_str}deg\")\n",
    "            try:\n",
    "                visualize_result(res) # Assumes visualize_result is defined in Cell 8\n",
    "                count_hierr += 1\n",
    "            except Exception as e:\n",
    "                 print(f\"  ERROR during visualization for {res.get('filename', 'N/A')}: {e}\")\n",
    "        else:\n",
    "            break # Stop after showing MAX_VIZ_EACH\n",
    "    if count_hierr == 0 and high_error_results:\n",
    "         print(\"\\nCould not visualize any of the available high-error results due to errors during visualization.\")\n",
    "    print(f\"\\nFinished displaying {count_hierr} high-error successful examples.\")\n",
    "\n",
    "# --- Visualize GENERALLY Successful Predictions (Optional: filter out high errors) ---\n",
    "# Option 1: Show first 5 overall successful (might include high errors again)\n",
    "# Option 2: Show first 5 successful that are *not* in high_error_results (shows low errors)\n",
    "# Let's go with Option 2 for clarity\n",
    "print(f\"\\n{'='*20} Visualizing up to {MAX_VIZ_EACH} LOW/MEDIUM Error Successful Prediction Examples {'='*20}\")\n",
    "\n",
    "if 'successful_results' not in locals() or not isinstance(successful_results, list):\n",
    "     print(\"ERROR: 'successful_results' list not found. Did you run the cell above (Cell 8.5)?\")\n",
    "elif not successful_results:\n",
    "     print(\"\\nNo successful predictions found at all.\")\n",
    "else:\n",
    "    # Filter out high-error examples already shown\n",
    "    low_med_error_results = [res for res in successful_results if res not in high_error_results]\n",
    "\n",
    "    if not low_med_error_results:\n",
    "         print(\"\\nNo successful predictions with low/medium error found (all successful had high error or failed visualization).\")\n",
    "    else:\n",
    "        print(f\"\\nDisplaying the first {min(len(low_med_error_results), MAX_VIZ_EACH)} low/medium-error successful examples...\")\n",
    "        count_success = 0\n",
    "        for res in low_med_error_results:\n",
    "            if count_success < MAX_VIZ_EACH:\n",
    "                coord_err_str = f\"{res.get('coord_error', 'N/A'):.2f}\" if res.get('coord_error') is not None else \"N/A\"\n",
    "                angle_err_str = f\"{res.get('angle_error', 'N/A'):.2f}\" if res.get('angle_error') is not None else \"N/A\"\n",
    "                print(f\"\\n[{count_success+1}/{min(len(low_med_error_results), MAX_VIZ_EACH)}] Visualizing LOW/MED ERROR result for: {res.get('filename', 'N/A')}\")\n",
    "                print(f\"  Errors - Coord: {coord_err_str}px, Angle: {angle_err_str}deg\")\n",
    "                try:\n",
    "                    visualize_result(res) # Assumes visualize_result is defined in Cell 8\n",
    "                    count_success += 1\n",
    "                except Exception as e:\n",
    "                     print(f\"  ERROR during visualization for {res.get('filename', 'N/A')}: {e}\")\n",
    "            else:\n",
    "                break # Stop after showing MAX_VIZ_EACH\n",
    "        if count_success == 0 and low_med_error_results:\n",
    "             print(\"\\nCould not visualize any of the available low/medium-error results due to errors during visualization.\")\n",
    "        print(f\"\\nFinished displaying {count_success} low/medium-error successful examples.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cbd6d04-d4d7-4c8b-aa26-f5d83fee1020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Definitions ---\n",
      "'matching' defined: True\n",
      "'config' defined: True\n",
      "'device' defined: True\n",
      "'QUERY_DIR' defined: True\n",
      "'REF_DIR' defined: True\n",
      "'read_image' defined: True\n",
      "'estimate_transform' defined: True\n",
      "'convert_to_relative_center_coords' defined: True\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 9.7: Checking Functions are loaded\n",
    "print(\"--- Checking Definitions ---\")\n",
    "print(f\"'matching' defined: {'matching' in locals()}\")\n",
    "print(f\"'config' defined: {'config' in locals()}\")\n",
    "print(f\"'device' defined: {'device' in locals()}\")\n",
    "print(f\"'QUERY_DIR' defined: {'QUERY_DIR' in locals()}\")\n",
    "print(f\"'REF_DIR' defined: {'REF_DIR' in locals()}\")\n",
    "print(f\"'read_image' defined: {'read_image' in locals()}\")\n",
    "print(f\"'estimate_transform' defined: {'estimate_transform' in locals()}\")\n",
    "print(f\"'convert_to_relative_center_coords' defined: {'convert_to_relative_center_coords' in locals()}\")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3552eb-101a-430b-bf3d-d3b36892deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Timing Prediction for 5 Random Image Pairs ---\n",
      "\n",
      "--- DEBUG: Pre-requisite checks passed. Proceeding with timing logic ---\n",
      "Selected 5 files for timing:\n",
      "  - (1721,12857)_(210.69,-113.71)_304.59_mumbai2.png\n",
      "  - (5374,2147)_(-228.25,-98.42)_276.33_mumbai2.png\n",
      "  - (6402,4204)_(56.41,119.81)_320.23_mumbai.png\n",
      "  - (4025,3684)_(-29.19,-116.47)_209.03_delhi.png\n",
      "  - (8005,15921)_(-54.09,26.92)_50.90_mumbai2.png\n",
      "\n",
      "--- Timing Results ---\n",
      "Successfully processed 5 out of 5 selected pairs.\n",
      "Total prediction time (sum of successful pairs): 1.408 seconds\n",
      "Average prediction time per successful pair:   0.282 seconds\n",
      "Total wall time for 5 attempts: 1.409 seconds\n",
      "\n",
      "--- End of Cell 10 execution ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Time Prediction for Random Sample (Corrected Version)\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2 # Ensure OpenCV is imported\n",
    "import math # Ensure math is imported\n",
    "from pathlib import Path\n",
    "import inspect # For checking functions/callables\n",
    "import traceback # For detailed error printing\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_SAMPLES_FOR_TIMING = 5\n",
    "# Make sure these paths are correct for your setup and match Cell 1\n",
    "QUERY_DIR = Path('./extended_dataset/camera image')\n",
    "REF_DIR = Path('./extended_dataset/reference image')\n",
    "\n",
    "print(f\"\\n--- Timing Prediction for {NUM_SAMPLES_FOR_TIMING} Random Image Pairs ---\")\n",
    "\n",
    "# --- Pre-requisite Checks ---\n",
    "# Check essential variables/functions defined in previous cells are available\n",
    "required_vars = ['matching', 'config', 'device', 'QUERY_DIR', 'REF_DIR']\n",
    "required_funcs = ['read_image', 'estimate_transform', 'convert_to_relative_center_coords'] # Note: estimate_transform isn't directly used below, but convert_... is\n",
    "all_required = required_vars + required_funcs\n",
    "\n",
    "missing_items = []\n",
    "for item_name in all_required:\n",
    "    if item_name not in locals():\n",
    "         missing_items.append(item_name)\n",
    "\n",
    "if missing_items:\n",
    "    print(\"\\nERROR: Missing required variables or functions.\")\n",
    "    missing_vars_found = [v for v in required_vars if v in missing_items]\n",
    "    missing_funcs_found = [f for f in required_funcs if f in missing_items]\n",
    "    if missing_vars_found:\n",
    "         print(f\"  Missing variables: {', '.join(missing_vars_found)}\")\n",
    "         print(\"  Please ensure Cells 1 and 5 were run successfully IN THIS KERNEL SESSION.\")\n",
    "    if missing_funcs_found:\n",
    "         print(f\"  Missing functions: {', '.join(missing_funcs_found)}\")\n",
    "         print(\"  Please ensure Cells defining these functions (SuperGlue utils, Cell 3, Cell 4) were run successfully IN THIS KERNEL SESSION.\")\n",
    "    print(\"\\nCannot proceed with timing.\")\n",
    "\n",
    "else:\n",
    "    # --- All checks passed, proceed with timing logic ---\n",
    "    print(\"\\n--- DEBUG: Pre-requisite checks passed. Proceeding with timing logic ---\")\n",
    "    # Use the variables confirmed to be in the local scope\n",
    "    matching = locals()['matching']\n",
    "    config = locals()['config']\n",
    "    device = locals()['device']\n",
    "    read_image = locals()['read_image']\n",
    "    # estimate_transform = locals()['estimate_transform'] # Not directly used below\n",
    "    convert_to_relative_center_coords = locals()['convert_to_relative_center_coords']\n",
    "\n",
    "    try:\n",
    "        # 1. Get list of valid image pairs\n",
    "        if not QUERY_DIR.is_dir() or not REF_DIR.is_dir():\n",
    "             raise FileNotFoundError(f\"Query ({QUERY_DIR}) or Reference ({REF_DIR}) image directory not found.\")\n",
    "\n",
    "        all_query_files = sorted([\n",
    "            f for f in QUERY_DIR.iterdir()\n",
    "            if f.is_file() and not f.name.startswith('.') and f.suffix.lower() == '.png'\n",
    "        ])\n",
    "\n",
    "        valid_pairs = []\n",
    "        for q_path in all_query_files:\n",
    "            r_path = REF_DIR / q_path.name\n",
    "            if r_path.exists():\n",
    "                valid_pairs.append(q_path)\n",
    "\n",
    "        if not valid_pairs:\n",
    "             print(\"No valid image pairs found in the dataset to perform timing.\")\n",
    "             num_to_process = 0\n",
    "        elif len(valid_pairs) < NUM_SAMPLES_FOR_TIMING:\n",
    "            print(f\"Warning: Only found {len(valid_pairs)} valid image pairs, \"\n",
    "                  f\"which is less than the requested {NUM_SAMPLES_FOR_TIMING}.\")\n",
    "            num_to_process = len(valid_pairs)\n",
    "        else:\n",
    "             num_to_process = NUM_SAMPLES_FOR_TIMING\n",
    "\n",
    "        if num_to_process > 0:\n",
    "            # 2. Select random sample\n",
    "            # random.seed(42) # Optional: for reproducible samples\n",
    "            selected_files = random.sample(valid_pairs, num_to_process)\n",
    "            print(f\"Selected {num_to_process} files for timing:\")\n",
    "            for f in selected_files:\n",
    "                print(f\"  - {f.name}\")\n",
    "\n",
    "            # 3. Run predictions and time\n",
    "            start_time = time.time() # Overall wall time start\n",
    "            processed_count = 0\n",
    "            prediction_times = [] # Store individual successful pair times\n",
    "\n",
    "            for query_path in selected_files:\n",
    "                filename = query_path.name\n",
    "                ref_path = REF_DIR / filename\n",
    "                pair_start_time = time.time() # Time each pair attempt\n",
    "\n",
    "                try:\n",
    "                    # --- Core Prediction Logic (Corrected Filtering) ---\n",
    "                    # Load Images\n",
    "                    image0, inp0, scales0 = read_image(str(query_path), device, [-1], 0, False) # Query\n",
    "                    image1, inp1, scales1 = read_image(str(ref_path), device, [-1], 0, False)   # Reference\n",
    "                    if image0 is None or image1 is None:\n",
    "                        print(f\"  Skipping {filename}: Image loading error.\")\n",
    "                        continue # Skip this pair\n",
    "                    query_h, query_w = image0.shape[:2]\n",
    "                    ref_h, ref_w = image1.shape[:2]\n",
    "\n",
    "                    # Perform Matching\n",
    "                    with torch.no_grad():\n",
    "                        pred = matching({'image0': inp0, 'image1': inp1})\n",
    "                    pred = {k: v[0].cpu().numpy() for k, v in pred.items()}\n",
    "                    kpts0, kpts1 = pred['keypoints0'], pred['keypoints1']\n",
    "                    matches, conf = pred['matches0'], pred['matching_scores0']\n",
    "\n",
    "                    # ** CORRECTED FILTERING **\n",
    "                    # Find indices where the match is valid AND confidence is above threshold\n",
    "                    good_match_indices = np.where((matches > -1) & (conf > config['superglue']['match_threshold']))[0]\n",
    "\n",
    "                    min_req_points = 3 # Minimum points for estimateAffine2D\n",
    "                    if len(good_match_indices) < min_req_points:\n",
    "                        print(f\"  Skipping {filename}: Not enough confident matches found ({len(good_match_indices)} < {min_req_points}).\")\n",
    "                        continue # Skip this pair\n",
    "\n",
    "                    # Get the coordinates of the keypoints for these good matches\n",
    "                    mkpts0_coords = kpts0[good_match_indices]       # Coords from image 0\n",
    "                    mkpts1_coords = kpts1[matches[good_match_indices]] # Coords from image 1\n",
    "\n",
    "                    # Estimate Transformation directly using the filtered coordinates\n",
    "                    M, mask = cv2.estimateAffine2D(mkpts0_coords, mkpts1_coords, method=cv2.RANSAC,\n",
    "                                                     ransacReprojThreshold=5.0) # RANSAC threshold (tune?)\n",
    "\n",
    "                    # Decompose M and check RANSAC inliers\n",
    "                    pred_angle = None\n",
    "                    trans_px_tl = None\n",
    "                    estimation_succeeded = False # Flag for success\n",
    "                    if M is not None:\n",
    "                        num_inliers = np.sum(mask) if mask is not None else 0\n",
    "                        if num_inliers < min_req_points:\n",
    "                            print(f\"  Skipping {filename}: RANSAC found only {num_inliers} inliers (less than {min_req_points}).\")\n",
    "                            # M = None # Keep M as None, estimation failed\n",
    "                        else:\n",
    "                            # RANSAC succeeded with enough inliers\n",
    "                            estimation_succeeded = True\n",
    "                            tx = M[0, 2]\n",
    "                            ty = M[1, 2]\n",
    "                            trans_px_tl = (tx, ty)\n",
    "                            angle_rad = math.atan2(M[1, 0], M[0, 0])\n",
    "                            angle_deg_raw = np.degrees(angle_rad)\n",
    "                            pred_angle = angle_deg_raw % 360\n",
    "                            if pred_angle < 0:\n",
    "                                pred_angle += 360\n",
    "                    else:\n",
    "                        print(f\"  Skipping {filename}: cv2.estimateAffine2D failed (returned None).\")\n",
    "                        # M remains None, estimation failed\n",
    "\n",
    "                    # If estimation failed (M is None or not enough inliers), skip pair\n",
    "                    if not estimation_succeeded:\n",
    "                        continue\n",
    "\n",
    "                    # Convert Coordinates (only if estimation succeeded)\n",
    "                    pred_x, pred_y = convert_to_relative_center_coords(\n",
    "                        trans_px_tl, pred_angle, (query_h, query_w), (ref_h, ref_w)\n",
    "                    )\n",
    "                    # --- End Core Prediction Logic ---\n",
    "\n",
    "                    # If we reach here, the pair was processed successfully\n",
    "                    pair_end_time = time.time()\n",
    "                    prediction_times.append(pair_end_time - pair_start_time)\n",
    "                    processed_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Print detailed error for the specific pair processing failure\n",
    "                    print(f\"  ERROR processing {filename} during timing loop:\")\n",
    "                    print(traceback.format_exc())\n",
    "                    # Exclude pairs that errored from average time calculation\n",
    "\n",
    "            end_time = time.time() # Overall wall time end\n",
    "\n",
    "            # 4. Calculate and report times\n",
    "            total_wall_time = end_time - start_time\n",
    "            if processed_count > 0:\n",
    "                 total_prediction_time = sum(prediction_times)\n",
    "                 average_time = total_prediction_time / processed_count\n",
    "                 print(f\"\\n--- Timing Results ---\")\n",
    "                 print(f\"Successfully processed {processed_count} out of {num_to_process} selected pairs.\")\n",
    "                 print(f\"Total prediction time (sum of successful pairs): {total_prediction_time:.3f} seconds\")\n",
    "                 print(f\"Average prediction time per successful pair:   {average_time:.3f} seconds\")\n",
    "                 print(f\"Total wall time for {num_to_process} attempts: {total_wall_time:.3f} seconds\") # Optional\n",
    "            else:\n",
    "                 print(\"\\nNo pairs were successfully processed during timing.\")\n",
    "                 print(f\"Total wall time for {num_to_process} attempts: {total_wall_time:.3f} seconds\") # Optional\n",
    "\n",
    "\n",
    "    except FileNotFoundError as fnf:\n",
    "        print(f\"ERROR during file operations: {fnf}\")\n",
    "    except NameError as ne:\n",
    "         # This check should have caught missing variables earlier\n",
    "         print(f\"UNEXPECTED ERROR: A required variable is not defined ({ne}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred OUTSIDE the timing loop:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "print(\"\\n--- End of Cell 10 execution ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649e3ac-86fc-42d2-8340-8063c0d06871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
